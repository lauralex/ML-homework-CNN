{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR-dBd9VHPoO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Training a CNN is not much different from training a MLP; you just have to change:\n",
    "\n",
    "- the way data are loaded;\n",
    "- the structure of the model;\n",
    "\n",
    "Let's train a CNN on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q1VZH1pHPoS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "id": "vv0GMtNlHPoT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root=\"..\", download=True, train=True)\n",
    "test_dataset = datasets.MNIST(root=\"..\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxkrjDUrHPoU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset interface: len\n",
    "print(f\"Num. training samples: {len(train_dataset)}\")\n",
    "print(f\"Num. test samples:     {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWyo_JjBHPoU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute dataset sizes\n",
    "num_train = len(train_dataset)\n",
    "num_test = len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh7FKm2yHPoV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's split our data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZjlhtvIHPoV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of indexes on the training set\n",
    "train_idx = list(range(num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfvyB4BaHPoW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of indexes on the test set\n",
    "test_idx = list(range(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH9i7fZ5HPoX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBu9vIAjHPoX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle training set\n",
    "random.shuffle(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvQpHsHjHPoY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Validation fraction\n",
    "val_frac = 0.1\n",
    "# Compute number of samples\n",
    "num_val = int(num_train*val_frac)\n",
    "num_train = num_train - num_val\n",
    "# Split training set\n",
    "val_idx = train_idx[num_train:]\n",
    "train_idx = train_idx[:num_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSWsB5i4HPoY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmenu41hHPoZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `DataLoader` class in `torch.utils.data` includes several useful functionalities for loading data.\n",
    "\n",
    "The constructor receives several arguments; the most important ones are:\n",
    "\n",
    "- `dataset`: input dataset;\n",
    "- `batch_size`: `DataLoader` automatically groups samples into batches (exactly as we did in the exercise);\n",
    "- `shuffle`: boolean for shuffling the dataset at each epoch. This is usually a good idea in training, while it doesn't matter for validation/test;\n",
    "- `num_workers`: number of background threads for data loading. This is useful when your computation time is large, so in the meantime you want to load data so that it's ready when the CPU/GPU is ready.\n",
    "- `drop_last`: when the number of samples is not divisable by the batch size, the last batch will have a number of elements smaller than `batch_size`. This parameters specifies if you want to use the last incomplete batch or not. Note that if `shuffle` is false, this means that you will never use the last samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cubgnLhHPoZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The problem with using `DataLoader` is that it requires a `Dataset` object. However, we only have two dataset instance, one for training and one for test, but we have three logical splits, because `train_dataset` is actually used -- with different indexes -- as a training set and validation set.\n",
    "\n",
    "To solve this problem, we can use the `torch.utils.data.Subset` class, which takes as input a `Dataset` and returns a new `Dataset` containing only the samples at the specified indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqF2OYmhHPoa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First of all, let's start by defining the transforms. This time, we will use a -1/1 representation(without standardization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLbGwYH8HPoa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import module\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0fBtmigHPoa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define single transforms\n",
    "\n",
    "# Note: transforms can also be regular functions\n",
    "def normalize_(x):\n",
    "    # Set values\n",
    "    x[x > 0.5] = 1\n",
    "    x[x <= 0.5] = -1\n",
    "    # Return\n",
    "    return x\n",
    "\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = normalize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuRfltq3HPoa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compose transforms\n",
    "transform = T.Compose([to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29KQkb-vHPob",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset with transforms\n",
    "train_dataset = datasets.MNIST(root=\"..\", download=True, train=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"..\", download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itv_o2JjHPob",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's split our training set into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOcBt2hFHPoc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rwTO3nOHPoc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split train_dataset into training and validation\n",
    "val_dataset = Subset(train_dataset, val_idx)\n",
    "train_dataset = Subset(train_dataset, train_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EK0qZfHPoc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, let's create the data loaders for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlGbD7DpHPoc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=0, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=8, num_workers=0, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=8, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sz2AQX4HPod",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionary of loaders\n",
    "loaders = {\"train\": train_loader,\n",
    "           \"val\": val_loader,\n",
    "           \"test\": test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYOUw3o7HPod",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CNN models\n",
    "\n",
    "The minimal layers we need for defining a CNN are:\n",
    "\n",
    "#### Convolutional layer\n",
    "\n",
    "`nn.Conv2d(in_features, out_features, kernel_size, stride, padding, dilation)`\n",
    "\n",
    "#### Non-linear activation\n",
    "\n",
    "- `nn.ReLU()` (module, e.g. to put in `nn.Sequential`)\n",
    "- `nn.functional.relu(x)` (function, e.g. to call in `forward()`)\n",
    "\n",
    "#### Max pooling\n",
    "\n",
    "- `nn.MaxPool2d(kernel_size, stride)` (module, e.g. to put in `nn.Sequential()`)\n",
    "- `nn.functional.max_pool2d(x, kernel_size, padding)` (function, e.g. to call in `forward()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy1MBza8HPod",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPwlXry9HPod",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main difficulty in choosing CNN parameters is to compute the size of the first fully-connected layer, which depends on the number of features maps and the size of each feature map at the last convolutional layer, which depend on the kernel sizes, padding, strides, dilations of all convolutional layers.\n",
    "\n",
    "The simplest thing to do is to add the convolutional layers first, see the output size, and then add the fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "id": "Wqb9fG24HPoe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define class\n",
    "class CNN_tmp(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Call parent constructor\n",
    "        super().__init__();\n",
    "        # Create convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "# Create the model\n",
    "model = CNN_tmp()\n",
    "# Get input\n",
    "test_x = train_dataset[0][0].unsqueeze(0)\n",
    "# Try forward\n",
    "out_size = model(test_x).size()\n",
    "print(f\"Out feature maps: {out_size} => out features: {out_size[1]*out_size[2]*out_size[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI3dRlsdHPoe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we know the size of the encoded CNN features, let's add the fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97fSKUO4HPoe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define class\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Call parent constructor\n",
    "        super().__init__();\n",
    "        # Create convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Create fully-connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # FC layer\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            # Classification layer\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x) # Bx256x4x4 (then, we will want -> Bx4096)\n",
    "        x = x.view(x.size(0), -1) # Bx4096\n",
    "        x = self.fc_layers(x) # Bx4096 -> Bx1024 -> Bx10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F61cDk-ZHPoe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpuzRf06HPof",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training function (we now use `DataLoader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeP8xVsxHPof",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, lr=0.001):\n",
    "    try:\n",
    "        # Create model\n",
    "        model = CNN()\n",
    "        print(model)\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                # Process each batch\n",
    "                for (input, labels) in loaders[split]:\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = model(input)\n",
    "                    loss = F.cross_entropy(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    pred_labels = pred.argmax(1)\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "AciuFT37HPof",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypnGXSrsHPof",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training is a bit slow... Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM1rTS9RHPog",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training batches\n",
    "print(f\"Num. training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ufg0bYJPHPog",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get a batch\n",
    "batch,labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LyrM_n5HPog",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print batch size (to check)\n",
    "print(batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv3vj0HcHPog",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# How much time does it take to process a batch?\n",
    "import time\n",
    "# Let's create a model\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZzTyOuCHPog",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute forward time\n",
    "start_time = time.time()\n",
    "out = model(batch)\n",
    "loss = F.cross_entropy(out,labels)\n",
    "end_time = time.time()\n",
    "forward_time = end_time - start_time\n",
    "print(f\"Forward time: {forward_time:.4f} seconds\")\n",
    "# Compute backward time\n",
    "start_time = time.time()\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "backward_time = end_time - start_time\n",
    "print(f\"Backward time: {backward_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcD0bUmcHPoh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recap\n",
    "print(f\"To process an epoch ({len(train_loader)} batches), it takes {len(train_loader)*(forward_time + backward_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "u7wJscYeHPoh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How to speed up computation? **CUDA**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo8eG6jfHPoh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PyTorch supports moving tensors and models from/to CPU and GPU, without having to change anything in the training code.\n",
    "\n",
    "In general, the best thing to do is:\n",
    "\n",
    "`dev = (\"cuda\" if torch.cuda.is_available() else \"cpu\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xutkomn2HPoh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select device\n",
    "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ootamBZsHPoh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we need to change our training function to move stuff to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gm6wKzSHPoh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, dev, lr=0.001):\n",
    "    try:\n",
    "        # Create model\n",
    "        model = CNN()\n",
    "        model = model.to(dev)\n",
    "        print(model)\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                # Process each batch\n",
    "                for (input, labels) in loaders[split]:\n",
    "                    # Move to CUDA\n",
    "                    input = input.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = model(input)\n",
    "                    loss = F.cross_entropy(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    _,pred_labels = pred.max(1)\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-CDBbXkHPoi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How do we test a model with CUDA? If you have a CUDA device, that's good!\n",
    "\n",
    "Otherwise, you can use [Google Colab](https://colab.research.google.com).\n",
    "\n",
    "It's a Jupyter-like environment, that allows you to train models with GPU support. Just remember to set `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daZAkYU7HPoi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check our training times on CUDA, also trying different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUW-HV3ZHPoi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Redefine train loader, to test batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=4, shuffle=True)\n",
    "# Let's get a batch\n",
    "batch,labels = next(iter(train_loader))\n",
    "batch = batch.to(dev)\n",
    "labels = labels.to(dev)\n",
    "# Let's create a model\n",
    "model = CNN()\n",
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhSwg6e9HPoi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute forward time\n",
    "start_time = time.time()\n",
    "out = model(batch)\n",
    "loss = F.cross_entropy(out,labels)\n",
    "end_time = time.time()\n",
    "forward_time = end_time - start_time\n",
    "print(f\"Forward time: {forward_time:.4f} seconds\")\n",
    "# Compute backward time\n",
    "start_time = time.time()\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "backward_time = end_time - start_time\n",
    "print(f\"Backward time: {backward_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDdWu3nCHPoi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recap\n",
    "print(f\"To process an epoch ({len(train_loader)} batches), it takes {len(train_loader)*(forward_time + backward_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTPpFzQAHPoi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recreate the loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, num_workers=4, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, num_workers=4, shuffle=False)\n",
    "# Define dictionary of loaders\n",
    "loaders = {\"train\": train_loader,\n",
    "           \"val\": val_loader,\n",
    "           \"test\": test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwkn7pBhHPoj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "train(100, dev, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q71rUWOLHPoj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "name": "cnn_mnist.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}