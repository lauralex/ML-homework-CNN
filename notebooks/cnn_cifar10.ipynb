{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9BmSLjVlF-p",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Let's try now a different dataset: **CIFAR10**\n",
    "\n",
    "CIFAR10 contains 60,000 images from the classes of real-life objects. PyTorch already supports downloading and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06r3l-FulF-u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "id": "SUJwD1VBlF-v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset\n",
    "train_dataset = datasets.CIFAR10(root=\".\", download=True, train=True)\n",
    "test_dataset = datasets.CIFAR10(root=\".\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCy9L5uHlF-w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset interface: len\n",
    "print(f\"Num. training samples: {len(train_dataset)}\")\n",
    "print(f\"Num. test samples:     {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kue014wIlF-x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute dataset sizes\n",
    "num_train = len(train_dataset)\n",
    "num_test = len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sMDUe63lF-x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset classes\n",
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0HTT1palF-y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get image size (\"size\" is a property of PIL.Image)\n",
    "train_dataset[0][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jz2ya_0BlF-y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show an image of a given class\n",
    "train_dataset[train_dataset.targets.index(3)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvuQSpCGlF-z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's split our data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxb8V7vblF-0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of indexes on the training set\n",
    "train_idx = list(range(num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJkVB42VlF-0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of indexes on the test set\n",
    "test_idx = list(range(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzXzeUd2lF-1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xiYida3lF-1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle training set\n",
    "random.shuffle(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKYnJm-ZlF-1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Validation fraction\n",
    "val_frac = 0.1\n",
    "# Compute number of samples\n",
    "num_val = int(num_train*val_frac)\n",
    "num_train = num_train - num_val\n",
    "# Split training set\n",
    "val_idx = train_idx[num_train:]\n",
    "train_idx = train_idx[:num_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jjAim_zlF-2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Image dataset transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FS4cpKSlF-2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `torchvision.transforms` module includes additional classes specific for image pre-processing. Some of them are:\n",
    "\n",
    "- `Resize`: resizes an image;\n",
    "- `RandomCrop`: randomly crops an image (data augmentation during training);\n",
    "- `RandomHorizontalFlip`: randomly flips an image (data augmentation during training);\n",
    "- `CenterCrop`: crops the central area of an image (used in testing, as counterpart to `RandomCrop`);\n",
    "- `Normalize`: performs standardization, given per-channel means and standard deviations.\n",
    "\n",
    "Usually, to do data augmentation, you crop an image to an area which is slightly smaller than the full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMSW3hwJlF-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import module\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1CyAy49lF-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define single transforms\n",
    "resize = T.Resize(32) # This won't do anything, since images are already at that size\n",
    "random_crop = T.RandomCrop(28) # train\n",
    "random_hor_flip = T.RandomHorizontalFlip() # train\n",
    "center_crop = T.CenterCrop(28) # test\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8gldDGWlF-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compose transforms\n",
    "train_transform = T.Compose([resize, random_crop, random_hor_flip, to_tensor, normalize])\n",
    "test_transform = T.Compose([resize, center_crop, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DDafFQtlF-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset with transforms\n",
    "train_dataset = datasets.CIFAR10(root=\".\", download=True, train=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root=\".\", download=True, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BOBVAsLlF-4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAyKwwM6lF-4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split train_dataset into training and validation\n",
    "val_dataset = Subset(train_dataset, val_idx)\n",
    "train_dataset = Subset(train_dataset, train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTUrS0LGlF-4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, num_workers=4, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXNKuj79lF-5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionary of loaders\n",
    "loaders = {\"train\": train_loader,\n",
    "           \"val\": val_loader,\n",
    "           \"test\": test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiqI4Ty_lF-5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `train` and `eval` modes\n",
    "\n",
    "Certain layers have different behaviours when used in training and in test/evaluation/production (*inference*).\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "During train, dropout randomly zeroes neuron outputs with probability $p$. During test, all neuron activations are used. However, this means that a neuron in the next layer, during training receives a fraction $1-p$ of the activations from the previous layer, but during test it receives all activations. This means that the total input to the neuron at test time is amplified by a factor of $\\frac{1}{1-p}$.\n",
    "\n",
    "To compensate, during test, the output of a dropout layer is multiplied by $1-p$. \n",
    "\n",
    "**Alternative**: scale during training, so you don't have to do anything special in inference.\n",
    "\n",
    "#### Batch normalization\n",
    "\n",
    "During training, batch normalization normalizes batches by their internal statistics. In this case, the larger the batch size, the better, because the statistics less stochastic. However, this also means that you need to have a batch to provide as input. PyTorch will raise an error if you try pass a single-element mini-batch to a batch normalization layer.\n",
    "\n",
    "In inference, sometimes you only want to process a single input. In this case, the common approach is as follows:\n",
    "\n",
    "- During training, keep track of batch statistics, and average the means and standard deviations.\n",
    "- During test, normalize input based on the average statistics (not on the input batch statistics).\n",
    "\n",
    "#### `train()` and `eval()`\n",
    "\n",
    "In order to specify which behaviour a layer should have, the generic `nn.Module` class includes the `train()` and `eval()` methods, which alter a layer's behaviour accordingly.\n",
    "\n",
    "For most layers (e.g. `nn.Linear`), there is no difference between the two modes, but for layers such as `Dropout2d` and `BatchNorm2d` it is important to set the layer to the correct working modality.\n",
    "\n",
    "In general, you can call `train()` and `eval()` on the full model, and it will take care of calling the corresponding functions on its layers and sub-modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WZ2NNC9lF-5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CNN model\n",
    "\n",
    "We will include **batch normalization** and **dropout**, thanks to the `nn.BatchNorm2d` and `nn.Dropout` modules. `BatchNorm2d` receives an argument that represents the number of *input channels*. `Dropout` is usually applied to fully-connected layers and receives as input the dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEjXUlsnlF-6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9MEGzMGlF-6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define class\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Call parent constructor\n",
    "        super().__init__();\n",
    "        # Create convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Create fully-connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # FC layer\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            # Classification layer\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8rrjuJ0lF-6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHcFHF4QlF-6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select device\n",
    "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IL-ZbwPUlF-7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, dev, lr=0.001):\n",
    "    try:\n",
    "        # Create model\n",
    "        model = CNN()\n",
    "        model = model.to(dev)\n",
    "        print(model)\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                # Process each batch\n",
    "                for i,(input, labels) in enumerate(loaders[split]):\n",
    "                    # Move to CUDA\n",
    "                    input = input.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = model(input)\n",
    "                    loss = F.cross_entropy(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    _,pred_labels = pred.max(1)\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lggahbx2lF-7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(100, dev, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khb3_CJRlF-7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "name": "cnn_cifar10.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}